import type { BaseMessage } from "@langchain/core/messages";
import { AIMessageChunk } from "@langchain/core/messages";
import type { ToolDefinition, BaseLanguageModelInput } from "@langchain/core/language_models/base";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { type BaseChatModelParams, BaseChatModel, LangSmithParams, BaseChatModelCallOptions } from "@langchain/core/language_models/chat_models";
import type { Tool as BedrockTool, GuardrailConfiguration } from "@aws-sdk/client-bedrock-runtime";
import { BedrockRuntimeClient } from "@aws-sdk/client-bedrock-runtime";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { DefaultProviderInit } from "@aws-sdk/credential-provider-node";
import type { DocumentType as __DocumentType } from "@smithy/types";
import { StructuredToolInterface } from "@langchain/core/tools";
import { Runnable, RunnableToolLike } from "@langchain/core/runnables";
import { ConverseCommandParams, CredentialType } from "./types.js";
import { BedrockConverseToolChoice } from "./common.js";
/**
 * Inputs for ChatBedrockConverse.
 */
export interface ChatBedrockConverseInput extends BaseChatModelParams, Partial<DefaultProviderInit> {
    /**
     * Whether or not to stream responses
     */
    streaming?: boolean;
    /**
     * Model to use.
     * For example, "anthropic.claude-3-haiku-20240307-v1:0", this is equivalent to the modelId property in the
     * list-foundation-models api.
     * See the below link for a full list of models.
     * @link https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns
     *
     * @default anthropic.claude-3-haiku-20240307-v1:0
     */
    model?: string;
    /**
     * The AWS region e.g. `us-west-2`.
     * Fallback to AWS_DEFAULT_REGION env variable or region specified in ~/.aws/config
     * in case it is not provided here.
     */
    region?: string;
    /**
     * AWS Credentials. If no credentials are provided, the default credentials from
     * `@aws-sdk/credential-provider-node` will be used.
     */
    credentials?: CredentialType;
    /**
     * Temperature.
     */
    temperature?: number;
    /**
     * Max tokens.
     */
    maxTokens?: number;
    /**
     * Override the default endpoint hostname.
     */
    endpointHost?: string;
    /**
     * The percentage of most-likely candidates that the model considers for the next token. For
     * example, if you choose a value of 0.8 for `topP`, the model selects from the top 80% of the
     * probability distribution of tokens that could be next in the sequence.
     * The default value is the default value for the model that you are using.
     * For more information, see the inference parameters for foundation models link below.
     * @link https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html
     */
    topP?: number;
    /**
     * Additional inference parameters that the model supports, beyond the
     * base set of inference parameters that the Converse API supports in the `inferenceConfig`
     * field. For more information, see the model parameters link below.
     * @link https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html
     */
    additionalModelRequestFields?: __DocumentType;
    /**
     * Whether or not to include usage data, like token counts
     * in the streamed response chunks. Passing as a call option will
     * take precedence over the class-level setting.
     * @default true
     */
    streamUsage?: boolean;
    /**
     * Configuration information for a guardrail that you want to use in the request.
     */
    guardrailConfig?: GuardrailConfiguration;
}
export interface ChatBedrockConverseCallOptions extends BaseChatModelCallOptions, Pick<ChatBedrockConverseInput, "additionalModelRequestFields" | "streamUsage"> {
    /**
     * A list of stop sequences. A stop sequence is a sequence of characters that causes
     * the model to stop generating the response.
     */
    stop?: string[];
    tools?: (StructuredToolInterface | ToolDefinition | BedrockTool)[];
    /**
     * Tool choice for the model. If passing a string, it must be "any", "auto" or the
     * name of the tool to use. Or, pass a BedrockToolChoice object.
     *
     * If "any" is passed, the model must request at least one tool.
     * If "auto" is passed, the model automatically decides if a tool should be called
     * or whether to generate text instead.
     * If a tool name is passed, it will force the model to call that specific tool.
     */
    tool_choice?: BedrockConverseToolChoice;
}
/**
 * Integration with AWS Bedrock Converse API.
 *
 * @example
 * ```typescript
 * import { ChatBedrockConverse } from "@langchain/aws";
 *
 * const model = new ChatBedrockConverse({
 *   region: process.env.BEDROCK_AWS_REGION ?? "us-east-1",
 *   credentials: {
 *     secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,
 *     accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,
 *   },
 * });
 *
 * const res = await model.invoke([new HumanMessage("Print hello world")]);
 * ```
 */
export declare class ChatBedrockConverse extends BaseChatModel<ChatBedrockConverseCallOptions, AIMessageChunk> implements ChatBedrockConverseInput {
    static lc_name(): string;
    /**
     * Replace with any secrets this class passes to `super`.
     * See {@link ../../langchain-cohere/src/chat_model.ts} for
     * an example.
     */
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_aliases(): {
        [key: string]: string;
    } | undefined;
    model: string;
    streaming: boolean;
    region: string;
    temperature?: number | undefined;
    maxTokens?: number | undefined;
    endpointHost?: string;
    topP?: number;
    additionalModelRequestFields?: __DocumentType;
    streamUsage: boolean;
    guardrailConfig?: GuardrailConfiguration;
    client: BedrockRuntimeClient;
    constructor(fields?: ChatBedrockConverseInput);
    getLsParams(options: this["ParsedCallOptions"]): LangSmithParams;
    bindTools(tools: (StructuredToolInterface | BedrockTool | ToolDefinition | Record<string, any> | RunnableToolLike)[], kwargs?: Partial<this["ParsedCallOptions"]>): Runnable<BaseLanguageModelInput, AIMessageChunk, this["ParsedCallOptions"]>;
    _llmType(): string;
    invocationParams(options?: this["ParsedCallOptions"]): Partial<ConverseCommandParams>;
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _generateNonStreaming(messages: BaseMessage[], options: Partial<this["ParsedCallOptions"]>, _runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
}
